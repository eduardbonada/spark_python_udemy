{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Example\n",
    "\n",
    "Logistic Regression example using the famous Titanic Dataset\n",
    "\n",
    "Other links to review:\n",
    "* https://docs.databricks.com/spark/latest/mllib/binary-classification-mllib-pipelines.html\n",
    "* https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spark and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('logred_titanic').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the input csv file.\n",
    "data = spark.read.csv('titanic.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "891"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only desired columns\n",
    "mycols_data = data.select('Survived','Pclass','Sex','Age','SibSp','Parch','Fare','Embarked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+---+-----+-----+----+--------+\n",
      "|Survived|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked|\n",
      "+--------+------+---+---+-----+-----+----+--------+\n",
      "|       0|     0|  0|177|    0|    0|   0|       2|\n",
      "+--------+------+---+---+-----+-----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list number of NANs or NULLs in each column\n",
    "from pyspark.sql.functions import count, when, isnan, col\n",
    "mycols_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in mycols_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all rows with a null value\n",
    "cleaned_data = mycols_data.na.drop()\n",
    "cleaned_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Categorical and Numerical Data with a Pipeline\n",
    "\n",
    "Transform categorical columns to OneHotEncoded (first String Indexing, and then One hot Encoding) and assemble the features vector, all that configured within a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,OneHotEncoder,StringIndexer)\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the transformations on categorical columns as stages in the Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all categorical values where we will apply the transformations\n",
    "categoricalColumns = ['Sex', 'Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the transformations on categorical columns as stages in the Pipeline\n",
    "stages = [] \n",
    "for categoricalCol in categoricalColumns:\n",
    "  # Category Indexing with StringIndexer\n",
    "  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n",
    "    \n",
    "  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"Vec\")\n",
    "    \n",
    "  # Add into stages\n",
    "  stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the vector assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all numerical values that need no transformation\n",
    "numericCols = ['Pclass','Age','SibSp','Parch','Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about normalization of numerical columns with a normalizer?\n",
    "#from pyspark.ml.feature import Normalizer\n",
    "#normalizer = Normalizer(inputCol=\"features\", outputCol=\"features_norm\", p=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the vector assembler add it as pipeline stage \n",
    "assemblerInputs = list(map(lambda c: c + \"Vec\", categoricalColumns)) + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_42ddbaec25f6043addb2,\n",
       " OneHotEncoder_4a11a4ed6b16bfa6fbf3,\n",
       " StringIndexer_43a786309767a32df370,\n",
       " OneHotEncoder_42ccbaf699903766ea99,\n",
       " VectorAssembler_4bae997d3bf54ff3477b]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the stages so far...\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline with all previous actions\n",
    "features_pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the feature transformations.\n",
    "#  - fit() computes feature statistics as needed.\n",
    "#  - transform() actually transforms the features.\n",
    "final_data = features_pipeline.fit(cleaned_data).transform(cleaned_data)\n",
    "\n",
    "# select only features and label columns\n",
    "final_data = final_data.select('features', 'Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 1.0, 0.0, 3.0, 22.0, 1.0, 0.0, 7.25]), Survived=0),\n",
       " Row(features=DenseVector([0.0, 0.0, 1.0, 1.0, 38.0, 1.0, 0.0, 71.2833]), Survived=1),\n",
       " Row(features=SparseVector(8, {1: 1.0, 3: 3.0, 4: 26.0, 7: 7.925}), Survived=1),\n",
       " Row(features=DenseVector([0.0, 1.0, 0.0, 1.0, 35.0, 1.0, 0.0, 53.1]), Survived=1),\n",
       " Row(features=DenseVector([1.0, 1.0, 0.0, 3.0, 35.0, 0.0, 0.0, 8.05]), Survived=0)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "# Randomly split data into training and test sets. set seed for reproducibility\n",
    "train_data, test_data = final_data.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(train_data.count())\n",
    "print(test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "log_reg = LogisticRegression(labelCol='Survived', featuresCol='features', maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "log_reg_model = log_reg.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = log_reg_model.transform(test_data)\n",
    "test_results.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+\n",
      "|Survived|prediction|         probability|\n",
      "+--------+----------+--------------------+\n",
      "|       1|       0.0|[0.89845272234831...|\n",
      "|       0|       0.0|[0.99421909288322...|\n",
      "|       0|       0.0|[0.99015289928703...|\n",
      "|       0|       0.0|[0.99071807477584...|\n",
      "|       0|       0.0|[0.99819441937833...|\n",
      "|       1|       1.0|[0.06127098232752...|\n",
      "|       1|       1.0|[0.03259240401987...|\n",
      "|       1|       1.0|[0.03851987436034...|\n",
      "|       1|       1.0|[0.04423406999497...|\n",
      "|       1|       1.0|[0.02292416975057...|\n",
      "|       1|       1.0|[0.17405298229200...|\n",
      "|       0|       1.0|[0.18491655095609...|\n",
      "|       1|       1.0|[0.18491655095609...|\n",
      "|       1|       1.0|[0.22130337442356...|\n",
      "|       1|       1.0|[0.23427956583345...|\n",
      "|       1|       1.0|[0.24795832655046...|\n",
      "|       0|       1.0|[0.27688037272054...|\n",
      "|       1|       1.0|[0.30779520830304...|\n",
      "|       0|       1.0|[0.44737225771414...|\n",
      "|       0|       1.0|[0.27283654818479...|\n",
      "+--------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.select('Survived','prediction','probability').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate with the BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "test_evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction',labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8024619960343694"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the area under the ROC curve\n",
    "test_evaluator.evaluate(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80% under the curve, not bad..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
