{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Consulting Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Binary Customer Churn\n",
    "\n",
    "A marketing agency has many customers that use their service to produce ads for the client/customer websites. They've noticed that they have quite a bit of churn in clients. They basically randomly assign account managers right now, but want you to create a machine learning model that will help predict which customers will churn (stop buying their service) so that they can correctly assign the customers most at risk to churn an account manager. Luckily they have some historical data, can you help them out? Create a classification algorithm that will help classify whether or not a customer churned. Then the company can test this against incoming data for future customers to predict which customers will churn and assign them an account manager.\n",
    "\n",
    "The data is saved as customer_churn.csv. Here are the fields and their definitions:\n",
    "\n",
    "    Name : Name of the latest contact at Company\n",
    "    Age: Customer Age\n",
    "    Total_Purchase: Total Ads Purchased\n",
    "    Account_Manager: Binary 0=No manager, 1= Account manager assigned\n",
    "    Years: Totaly Years as a customer\n",
    "    Num_sites: Number of websites that use the service.\n",
    "    Onboard_date: Date that the name of the latest contact was onboarded\n",
    "    Location: Client HQ Address\n",
    "    Company: Name of Client Company\n",
    "    \n",
    "Once you've created the model and evaluated it, test out the model on some new data (you can think of this almost like a hold-out set) that your client has provided, saved under new_customers.csv. The client wants to know which customers are most likely to churn given this data (they don't have the label yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Spark and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('logreg_project').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the input csv file.\n",
    "data = spark.read.csv('customer_churn.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data\n",
    "\n",
    "Just select the columns we are interested in and check how many nulls are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only desired columns\n",
    "mycols_data = data.select('Age','Total_Purchase','Years','Num_Sites','Company','Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----+---------+-------+-----+\n",
      "|Age|Total_Purchase|Years|Num_Sites|Company|Churn|\n",
      "+---+--------------+-----+---------+-------+-----+\n",
      "|  0|             0|    0|        0|      0|    0|\n",
      "+---+--------------+-----+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list number of NANs or NULLs in each column\n",
    "from pyspark.sql.functions import count, when, isnan, col\n",
    "mycols_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in mycols_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing to drop\n",
    "cleaned_data = mycols_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------------+------+---------+--------------------+------+\n",
      "|summary|   Age|Total_Purchase| Years|Num_Sites|             Company| Churn|\n",
      "+-------+------+--------------+------+---------+--------------------+------+\n",
      "|  count|900.00|        900.00|900.00|   900.00|                 900|900.00|\n",
      "|   mean| 41.82|     10,062.82|  5.27|     8.59|                null|  0.17|\n",
      "| stddev|  6.13|      2,408.64|  1.27|     1.76|                null|  0.37|\n",
      "|    min| 22.00|        100.00|  1.00|     3.00|     Abbott-Thompson|  0.00|\n",
      "|    max| 65.00|     18,026.01|  9.15|    14.00|Zuniga, Clark and...|  1.00|\n",
      "+-------+------+--------------+------+---------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check stats of each column with a pretty printed describe()\n",
    "\n",
    "from pyspark.sql.functions import format_number\n",
    "cdd = cleaned_data.describe()\n",
    "cdd.select([cdd['summary'],\n",
    "            format_number(cdd['Age'].cast('double'), 2).alias('Age'),\n",
    "            format_number(cdd['Total_Purchase'].cast('double'), 2).alias('Total_Purchase'),\n",
    "            format_number(cdd['Years'].cast('double'), 2).alias('Years'),\n",
    "            format_number(cdd['Num_Sites'].cast('double'), 2).alias('Num_Sites'),\n",
    "            cdd['Company'],\n",
    "            format_number(cdd['Churn'].cast('double'), 2).alias('Churn'),\n",
    "           ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many churns?\n",
    "cleaned_data.filter(cleaned_data['Churn'] == 1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "\n",
    "All columns are numerical except 'Company', to which we will apply a string indexation (one hop encoding nedded here? i'll skip it).\n",
    "\n",
    "Then we will assemble the features vector.\n",
    "\n",
    "All this configured in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,StringIndexer)\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all string columns where we will apply the transformations\n",
    "string_columns = ['Company']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the transformations on categorical columns as stages in the Pipeline\n",
    "stages = [] \n",
    "for col in string_columns:\n",
    "  # Category Indexing with StringIndexer\n",
    "  stringIndexer = StringIndexer(inputCol=col, outputCol=col+\"Index\")\n",
    "    \n",
    "  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "  #encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"Vec\")\n",
    "    \n",
    "  # Add into stages\n",
    "  stages += [stringIndexer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all numerical features that need no transformation\n",
    "numeric_colums = ['Age','Total_Purchase','Years','Num_Sites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about normalization of numerical columns with a normalizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the vector assembler add it as pipeline stage \n",
    "assembler_inputs = list(map(lambda c: c + \"Index\", string_columns)) + numeric_colums\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_40fdaf0947592adb3a13, VectorAssembler_4f96a395ebf9de348253]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the stages so far...\n",
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pipeline with all previous actions\n",
    "features_pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the feature transformations\n",
    "final_data = features_pipeline.fit(cleaned_data).transform(cleaned_data)\n",
    "\n",
    "# select only features and label columns\n",
    "final_data = final_data.select('features', 'Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([824.0, 42.0, 11066.8, 7.22, 8.0]), Churn=1),\n",
       " Row(features=DenseVector([1.0, 41.0, 11916.22, 6.5, 11.0]), Churn=1),\n",
       " Row(features=DenseVector([272.0, 38.0, 12884.75, 6.67, 12.0]), Churn=1),\n",
       " Row(features=DenseVector([21.0, 42.0, 8010.76, 6.71, 10.0]), Churn=1),\n",
       " Row(features=DenseVector([524.0, 37.0, 9191.58, 5.56, 9.0]), Churn=1)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618\n",
      "282\n"
     ]
    }
   ],
   "source": [
    "# Randomly split data into training and test sets. set seed for reproducibility\n",
    "train_data, test_data = final_data.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(train_data.count())\n",
    "print(test_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Logistic Regression model\n",
    "\n",
    "Just fit the model with the train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "log_reg = LogisticRegression(labelCol='Churn', featuresCol='features', maxIter=20)\n",
    "\n",
    "# Train model with Training Data\n",
    "log_reg_model = log_reg.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45315607565403704,\n",
       " 0.4502984696058951,\n",
       " 0.4320218479136546,\n",
       " 0.3712197048313319,\n",
       " 0.3645315593854564,\n",
       " 0.36315719238575317,\n",
       " 0.36296060505013067,\n",
       " 0.3626645236647848,\n",
       " 0.36189973320469954,\n",
       " 0.3600359063097216,\n",
       " 0.3556504981285021,\n",
       " 0.346548428413119,\n",
       " 0.33091381368477735,\n",
       " 0.32206015312551034,\n",
       " 0.3053964968577326,\n",
       " 0.2937744858197606,\n",
       " 0.28883723135422795,\n",
       " 0.28362863146181316,\n",
       " 0.25963878894581965,\n",
       " 0.2572683493594377,\n",
       " 0.2570045126178582]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe history of objective (optimizer)\n",
    "log_reg_model.summary.objectiveHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|              Churn|         prediction|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|                618|                618|\n",
      "|   mean|0.16828478964401294|0.13268608414239483|\n",
      "| stddev| 0.3744220438216086|0.33950994595753775|\n",
      "|    min|                0.0|                0.0|\n",
      "|    max|                1.0|                1.0|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# stats on predictions vs labels (on train data)\n",
    "log_reg_model.summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test data\n",
    "\n",
    "Now evaluate the fitted model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = log_reg_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|Churn|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|    0|       0.0|[0.94539994899230...|\n",
      "|    0|       0.0|[0.94025049244233...|\n",
      "|    0|       0.0|[0.94922647221510...|\n",
      "|    0|       0.0|[0.90177178049092...|\n",
      "|    1|       1.0|[0.15486082733329...|\n",
      "|    0|       0.0|[0.99789117379955...|\n",
      "|    0|       0.0|[0.92654019294094...|\n",
      "|    0|       0.0|[0.94066404686597...|\n",
      "|    0|       0.0|[0.90740094916262...|\n",
      "|    1|       1.0|[0.17999244510170...|\n",
      "|    0|       0.0|[0.98552435893210...|\n",
      "|    0|       0.0|[0.90126407756561...|\n",
      "|    0|       0.0|[0.83898347775944...|\n",
      "|    0|       0.0|[0.99577290879145...|\n",
      "|    0|       0.0|[0.98274591902442...|\n",
      "|    0|       0.0|[0.94615249194316...|\n",
      "|    0|       0.0|[0.65823913194111...|\n",
      "|    0|       0.0|[0.63800169380683...|\n",
      "|    0|       0.0|[0.99275343419509...|\n",
      "|    0|       0.0|[0.96465727920270...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.select('Churn','prediction','probability').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "test_evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction',labelCol='Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8866985998526159"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the area under ROC curve\n",
    "test_evaluator.evaluate(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on unlabeled data\n",
    "\n",
    "Import the file of new customers and apply the features transformation pipeline to construct the features vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the input csv file.\n",
    "unlabeled_data = spark.read.csv('new_customers.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the feature transformations\n",
    "unlabeled_final_data = features_pipeline.fit(new_data).transform(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- CompanyIndex: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unlabeled_final_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the model and predict 'Churn'\n",
    "churn_predictions = log_reg_model.transform(unlabeled_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+\n",
      "|         Company|prediction|\n",
      "+----------------+----------+\n",
      "|        King Ltd|       0.0|\n",
      "|   Cannon-Benson|       1.0|\n",
      "|Barron-Robertson|       1.0|\n",
      "|   Sexton-Golden|       1.0|\n",
      "|        Wood LLC|       0.0|\n",
      "|   Parks-Robbins|       1.0|\n",
      "+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "churn_predictions.select('Company','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
